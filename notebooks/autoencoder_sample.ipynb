{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST codeをworm2vec用に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Ghassen Hamrouni\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WormDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    training_dir = '201302081337/main'\n",
    "    test_dir = '201302081353/main'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "\n",
    "        self.root = root    # root_dir \\Tanimoto_eLife_Fig3B or \\unpublished control\n",
    "        self.train = train  # training set or test set\n",
    "        self.transform = transform\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.')\n",
    "\n",
    "        if self.train:\n",
    "            data_dir = self.training_dir\n",
    "        else:\n",
    "            data_dir = self.test_dir\n",
    "\n",
    "        self.data = glob.glob(self.root + data_dir + \"/*\")\n",
    "        self.targets = self.data.copy()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where image == target.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        img = Image.open(img)\n",
    "        target = Image.open(target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            target = self.transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _check_exists(self):\n",
    "        print(self.root + self.training_dir)\n",
    "        return (os.path.exists(self.root + self.training_dir) and\n",
    "                os.path.exists(self.root + self.test_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "worm_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "F:\\Tanimoto_eLife_Fig3B\\201302081337/main\nF:\\Tanimoto_eLife_Fig3B\\201302081337/main\n"
    }
   ],
   "source": [
    "train_set = WormDataset(root=\"F:\\Tanimoto_eLife_Fig3B\\\\\", train=True,\n",
    "    transform=worm_transforms)\n",
    "\n",
    "test_set = WormDataset(root=\"F:\\Tanimoto_eLife_Fig3B\\\\\", train=False,\n",
    "    transform=worm_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=256, shuffle=True)\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vae model from https://github.com/podgorskiy/VAE\n",
    "\"\"\"\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, zsize, layer_count=3, channels=3):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        d = 128\n",
    "        self.d = d\n",
    "        self.zsize = zsize\n",
    "\n",
    "        self.layer_count = layer_count\n",
    "\n",
    "        mul = 1\n",
    "        inputs = channels\n",
    "        for i in range(self.layer_count):\n",
    "            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, d * mul, 3, 2, 1))\n",
    "            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(d * mul))\n",
    "            inputs = d * mul\n",
    "            mul *= 2\n",
    "\n",
    "        self.d_max = inputs\n",
    "\n",
    "        self.fc1 = nn.Linear(inputs * 4 * 4, zsize)\n",
    "        self.fc2 = nn.Linear(inputs * 4 * 4, zsize)\n",
    "\n",
    "        self.d1 = nn.Linear(zsize, inputs * 4 * 4)\n",
    "\n",
    "        mul = inputs // d // 2\n",
    "\n",
    "        for i in range(1, self.layer_count):\n",
    "            setattr(self, \"deconv%d\" % (i + 1), nn.ConvTranspose2d(inputs, d * mul, 3, 2, 1, 1))\n",
    "            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(d * mul))\n",
    "            inputs = d * mul\n",
    "            mul //= 2\n",
    "\n",
    "        setattr(self, \"deconv%d\" % (self.layer_count + 1), nn.ConvTranspose2d(inputs, channels, 3, 2, 1, 1))\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        for i in range(self.layer_count):\n",
    "            x = F.relu(getattr(self, \"conv%d_bn\" % (i + 1))(getattr(self, \"conv%d\" % (i + 1))(x)))\n",
    "\n",
    "        x = x.view(x.shape[0], self.d_max * 4 * 4)\n",
    "        h1 = self.fc1(x)\n",
    "        h2 = self.fc2(x)\n",
    "        return h1, h2\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = x.view(x.shape[0], self.zsize)\n",
    "        x = self.d1(x)\n",
    "        x = x.view(x.shape[0], self.d_max, 4, 4)\n",
    "        #x = self.deconv1_bn(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        for i in range(1, self.layer_count):\n",
    "            x = F.leaky_relu(getattr(self, \"deconv%d_bn\" % (i + 1))(getattr(self, \"deconv%d\" % (i + 1))(x)), 0.2)\n",
    "\n",
    "        x = torch.tanh(getattr(self, \"deconv%d\" % (self.layer_count + 1))(x))\n",
    "#        x = F.tanh(getattr(self, \"deconv%d\" % (self.layer_count + 1))(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z.view(-1, self.zsize, 1, 1)), mu, logvar\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "VAE(\n  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv1_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (conv4_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=16384, out_features=64, bias=True)\n  (fc2): Linear(in_features=16384, out_features=64, bias=True)\n  (d1): Linear(in_features=64, out_features=16384, bias=True)\n  (deconv2): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (deconv2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (deconv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv4): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (deconv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv5): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "z_size = 64\n",
    "vae = VAE(zsize=z_size, layer_count=4, channels=1)\n",
    "if device == \"cuda\":\n",
    "    vae.cuda()\n",
    "#vae.weight_init(mean=0, std=0.02)\n",
    "vae.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(vae.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = torch.mean((recon_x - x)**2)\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.mean(torch.mean(1 + logvar - mu.pow(2) - logvar.exp(), 1))\n",
    "        return BCE, KLD * 0.1\n",
    "\n",
    "    vae.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, _ = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        rec, mu, logvar = vae(data)\n",
    "        loss_re, loss_kl = loss_function(rec, data, mu, logvar)\n",
    "        (loss_re + loss_kl).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss_re: {:.6f} \\tLoss_kl: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss_re.item(), loss_kl.item()))\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train Epoch: 1 [0/3370 (0%)]\tLoss_re: 1.555981 \tLoss_kl: 0.012176\nTrain Epoch: 2 [0/3370 (0%)]\tLoss_re: 0.944288 \tLoss_kl: 0.009241\nTrain Epoch: 3 [0/3370 (0%)]\tLoss_re: 0.642188 \tLoss_kl: 0.010324\nTrain Epoch: 4 [0/3370 (0%)]\tLoss_re: 0.510253 \tLoss_kl: 0.010604\nTrain Epoch: 5 [0/3370 (0%)]\tLoss_re: 0.446586 \tLoss_kl: 0.009689\nTrain Epoch: 6 [0/3370 (0%)]\tLoss_re: 0.413263 \tLoss_kl: 0.008546\nTrain Epoch: 7 [0/3370 (0%)]\tLoss_re: 0.394048 \tLoss_kl: 0.007345\nTrain Epoch: 8 [0/3370 (0%)]\tLoss_re: 0.378823 \tLoss_kl: 0.006707\nTrain Epoch: 9 [0/3370 (0%)]\tLoss_re: 0.367194 \tLoss_kl: 0.006344\nTrain Epoch: 10 [0/3370 (0%)]\tLoss_re: 0.358658 \tLoss_kl: 0.005972\nTrain Epoch: 11 [0/3370 (0%)]\tLoss_re: 0.350836 \tLoss_kl: 0.005813\nTrain Epoch: 12 [0/3370 (0%)]\tLoss_re: 0.346329 \tLoss_kl: 0.005478\nTrain Epoch: 13 [0/3370 (0%)]\tLoss_re: 0.341501 \tLoss_kl: 0.005530\nTrain Epoch: 14 [0/3370 (0%)]\tLoss_re: 0.331296 \tLoss_kl: 0.005496\nTrain Epoch: 15 [0/3370 (0%)]\tLoss_re: 0.325126 \tLoss_kl: 0.005485\n"
    }
   ],
   "source": [
    "for epoch in range(1, 15 + 1):\n",
    "    train(epoch)\n",
    "    #test()\n",
    "\n",
    "torch.save(vae.state_dict(), \"models/VAEmodel.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "vae = VAE(zsize=z_size, layer_count=4, channels=1)\n",
    "vae.load_state_dict(torch.load(\"models/VAEmodel.pkl\"))\n",
    "\n",
    "def evaluation(vae, eval_id):\n",
    "    if not os.path.exists('results/' + eval_id):\n",
    "        os.mkdir('results/' + eval_id)\n",
    "    vae.eval()\n",
    "\n",
    "    z_size = 64\n",
    "    im_size = 64\n",
    "    sample_v = torch.randn(128, z_size).view(-1, z_size, 1, 1)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        x = data\n",
    "        x_rec, _, _ = vae.forward(x)\n",
    "        resultsample = torch.cat([x, x_rec]) * 0.5 + 0.5\n",
    "        resultsample = resultsample.cpu()\n",
    "        save_image(resultsample.view(-1, 1, im_size, im_size),\n",
    "                    'results/'+ eval_id +'/sample_encode.png')\n",
    "\n",
    "        x_rec = vae.decode(sample_v)\n",
    "        resultsample = x_rec * 0.5 + 0.5\n",
    "        resultsample = resultsample.cpu()\n",
    "        save_image(resultsample.view(-1, 1, im_size, im_size),\n",
    "                    'results/'+ eval_id +'/sample_decode.png')\n",
    "        break\n",
    "\n",
    "eval_id = \"002\"\n",
    "evaluation(vae, eval_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}